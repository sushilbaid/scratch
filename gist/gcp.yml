- envs: # environment variables used in other commands
  - export P="$(gcloud config get project)"  
  - export A="gkeops@${P}.iam.gserviceaccount.com"
- config:
    examples:
    - config configurations create # create named configuration
    - config configurations describe default # describe a named configuration
    - config get core/project # give value of project property under core section of config
    - config set project <project> # set core section property
    - config set artifacts/location us-central1 # sets artifacts location property
- logging:
    notes:
    - billing - 0.5usd per gb one time streaming cost. first 50gb streaming free per month. retention cost 0.01 per gb per month after first month.
    - no additional cost for analytics query, ingress, log routing
    examples:
    - logging write alog "logging - test1" # logs text log to log named "alog"
    - logging write alog --payload-type=json # logs json payload to log named alog -- '{"p1": "p1-value", "p2":"p2value"}'
    - logging read "logName=projects/$P/logs/alog" # gives logs based on the given filter in default yaml format
- bigtable:
    tablet: rows are sharded into contigous rows, called tablets. similar to regions in hbase.
    node: earlier called tablet server, is part of a cluster. cluster with more number of nodes can process more requests simultaneously. if replication 
     is enabled between multiple clusters, it can allow to route traffic to different clusters, enable failover etc.
    column family: related columns are grouped by column family. column is identified by family and column qualifier
    cell: each (row, column) contains multiple cells. cell contains timestamped data.
    collossus: google file system used to store tablets data. it is stored in sstable format. data is written to shared log.
    load balancing: tablet can be split or tablets may be merged based on workload. 
    notes:
    - table contains key/value pairs and massively scalable. 
    - table is compacted periodically. at that time - deletion storage is reclaimed.
    - compression of data less than 1mb is done and works best if it is placed next to each other. 
    - single cluster is strongly consistent. multiple clusters is eventual consistent. read-your-writes consistency may be used to read latest written data.
    -  in such case apps needs to be routed to single cluster using app profile.
    - in case of replication conflict between clusters, last-write-win resolution is used
    - backups allows schema and data to be backed up and restored to new table at a later time.
    - data is stored encrypted at rest. CMEK (customer managed encryption keys) can be used.
    - security can be managed at level of instance, and table. row level and downwards is not supported.
    - perf - for burst load autoscaling will not help. min worker config can be increased. bad schema design can not be addressed by changing number of nodes.
    -  in such cases, there may be hotspot nodes.
    - with multiple cluster using app profiles - traffic may be routed e.g. for batch jobs and app to different clusters, or route to nearest cluster
    - 
    examples:
    - cbt createinstance i1 "i1 instance" i1-c us-central1 ssd # create instance i1 with display name as "i1 instance" and single cluster i1-c in us-central1 region with ssd storage type
    - edit ~/.cbtrc file with project=<project> and instance=i1
    - cbt createtable t1 # table t1 in instance i1
    - cbt createfamily t1 cf1 # create cf1 column family
    - cbt set t1 r1 cf1:c1=c1-value # set (r1, cf1:c1) to c1-value
    - cbt read t1 regex=r[1-9]* columns=cf1:c1 # read t1 content for rowkey matching regex and columns 
    - cbt deletetable t1 # delete table t1
    - cbt deleteinstance i1 # delete instance i1
- bigquery:
    notes:
    - data analytics platform. allow to create datasets, query tables in datasets and export data.
    - billing is for compute(analysis) and storage need.
    - compute billing is 
    - 1. on demand (bytes processed). depends on the size of columns and rows scanned. clustering and partitioning of tables can help.
    -  quotas may be used per user, project, query. first tb is free. thereafter per tb around usd 6.
    - 2. capacity commitment based. charged based on slot hr. e.g. 0.04 slot hr. editions - standard, enterprise, enterprise plus. 
    - have optional reservation for 1 yr/3 yr with discounts for slot hr billing rate.
    - storage billing is based on active vs long term storage. table partitions not modified for 90 days are considered longterm storage.
    - first 10Gib is free per month. 
- iam:
    name: identity and access management
    principal: email with relevant prefix (e.g. user, serviceAccount, group, )
    permission: operation allowed
    role: collection of permissions (operations allowed) on a resource
    policy: collection of (role, principal) bindings attached to a resource
    examples:
    - iam service-accounts create abc123 # create service account
    - iam service-accounts delete abc123@$P.iam.gservieaccounts.com # delete service account
    - projects get-iam-policy <project> - list policy for a project
    - projects add-iam-policy-binding <project> --member="serviceAccount:$A" --role=roles/artifactregistry.writer
    - iam service-accounts get-iam-policy gkeops@$P.iam.gserviceaccount.com - list policy for the service account
    - iam workload-identity-pools create pool1 --location=global # create workload identity pool
    - iam workload-identity-pools providers create-oidc github-provider --location=global --project=$P --workload-identity-pool=pool1 --display-name="github identity provider" --attribute-mapping="google.subject=assertion.sub,attribute.actor=assertion.actor,attribute.repository=assertion.repository,attribute.repository_owner=assertion.repository_owner" --issuer-uri="https://token.actions.githubusercontent.com" # create oidc workload identity provider
    - iam service-accounts add-iam-policy-binding $A --member="principalSet://iam.googleapis.com/projects/<projectid>/locations/<location>/workloadIdentityPools/pool1/attribute.repository_owner/<owner>" --role=roles/iam.workloadIdentityUser --project=$P 
      adds role binding to service account resource
- container:
    examples:
    - container clusters create-auto c1 --region=us-central1 # create an autopilot gke cluster
    - container clusters get-credentials c1 --region=us-central1 # fetch credentials for the cluster c1
- services:
    name: services management
    examples:
    - services list --available # list services available
    - services list --enabled # list services enabled
    - services enable cloudfunctions.googleapis.com # enables the service in the default project set by gcloud config set project
- functions:
    name: cloud functions
    examples:
    - create go files in the current directory # as per this sample https://github.com/GoogleCloudPlatform/golang-samples/tree/main/functions/functionsv2/helloworld
    - functions deploy go-http-function --trigger-http --runtime=go120 --gen2 --region=us-central1 --source=. --entry-point=HelloGet --allow-unauthenticated # deploy go-http-function
    - functions describe go-http-function # gives details of the function including url
    - curl <function-url>
- storage:
    name: google storage
    object: identified by name and generation number is generated by gc. having randomness in name (instead of sequenceness) helps distribute load and 
      auto-scale read/write well from otherwise default 5000/1000 read/write per second. e.g. my-bucket/2fa764-2016-05-10-12-00-00/file1.
      note the 6 character hash prefix before sequencial timestamp prefix. virtual hiearchy is supported by the cli; there is no sub-directory in bucket.
      hence deeply nested directory lookup will not perform well. object are replaced atomically.
    security: 
    - iam based policy can be applied at bucket level (recommended). else acls at level of objects can be applied.
    - once uniform access based on iam is enabled, acls based access is disabled. it can be reverted upto 90 days. 
    - public access can be granted using role/storageView for allusers (iam policy). similarly for acls.
    lifecycle:
    - lifecycle config can be set for the bucket. it is a collection of rules. rule is a pair of action & conditition. 
    - rule allows to delete, set storage class and abort incomplete multipart uploads
    - conditions can be age since creation time, since non current time, num of versions etc.
    - sample config file and details https://cloud.google.com/storage/docs/lifecycle-configurations
    - object may be put on hold. on such objects, retention lifecycle policy is not applied.
    examples:
    - storage objects update gs://bucket-a/Dockerfile --add-acl-grant=entity=allUsers,role=READER # give access to all users
    - storage objects update gs://bucket-a/Dockerfile --remove-acl-grant=allUsers,role=READER # remove access for all users
    - storage buckets describe gs://bucket-a --format="default(default_acl)" # check default acls defined at bucket level
    - storage buckets update gs://bucket-a --uniform-bucket-level-access  #enable uniform bucket level iam based access
    - storage buckets update gs://bucket-a --no-uniform-bucket-level-access  #disable uniform bucket level iam based access
    - storage buckets update gs://bucket-a --lifecycle-file=lifecyclefile.json # update lifecycle config for the bucket
    - storage buckets update gs://bucket-a --clear-lifecycle # clear lifecycle config
    - export B=gs://sushil-baid-temp
    - storage buckets create $B # create bucket
    - storage buckets create gs://sushil-baid-a #another bucket
    - storage cp -r . $B
    - storage cp -r $B gs://sushil-baid-a
    - storage buckets update $B --update-labels=tag1=v1 # add label
    - storage buckets update $B --clear-labels #clear all labels
    - storage buckets describe $B --format="value(labels)"
    - storage rm -r $B # remove content and bucket
    - storage rm --all-versions "$B"/** # remove all content but bucket
    client-library-go-examples:
    - tbd
    client-library-python-examples:
    - tbd
    upload-downloads:
    - tbd
    signed-url:
    - tbd
    migrate-from-aws:
    - tbd
    monitoring:
    - tbd
    enryptions:
    - encryption may use customer supplied keys and client side encryption in addition.
    - tbd
- secret-manager:
    notes:
    - manages secrets like password, credentials. can be auto-rotated using cloud functions. latest version may be pinned to a version for requests. 
    - audit logs for ops. replicated to selected regions. only project owners have default access.
    - access roles - roles/secretmanager.admin, secretAccessor, secretVersionAdder, secretVersionManager, viewer
    - keeping secret name and version in app config is one good practice to avoid service downtime because of bad secret rollout.
    - secret scheduled rollout enables system with process to deal with any emergency that requires secret rotation.
    - can send message pubsub topic based on rotation-period
    examples:
    - echo -n "secret data" | gcloud secrets create a --data-file=- # create secret
    - secrets versions access latest --secret=a # access latest/version of secret a
    - secrets update a --next-rotation-time 2021-06-01T09:00:00Z --rotation-period="2592000s" --add-topics="secret-expiry" # notifies for rotation
    - secrets update a --remove-next-rotation-time
    - secrets update a --remove-rotation-schedule
- networking:
    notes:
    - VPC is virtual private cloud and is based on software defined network. uses private networking and ip addressing standard (rfc 1918).
    - not exposed to internet. ingres is free. egress traffic has a cost. gcp resources uses the google networking infra. it includes 20 regions. 
    - region includes multiple zones. POP (edge point of presence) is where google's network connect to rest of internet. over 130 exchange points.
    - all traffic between gcp resources across regions is over google's private network. never touch public internet. better security, routing, perf.
    - regions - geographical areas that host gcp data centers. zones - deployment areas for gcp resources within a region. 
    - projects separate users where as VPCs separate systems. by default project can have upto 5 vpc. quota may be increased.
    - subnets defined using cidr notation for network/host division. private ip ranges (172.16.x.x/10.x.x.x/192.168.x.x). 
    - subnetting: 
      - dividing network address space. subnetting is logical partitioning of network. subnet may span zones in a region. e.g. us-central1-a/b/c. 
      - subnet modes - default, auto mode, custom. default is automode + premade firewall rules. automode creates subnets for all 20 regions.
      - custom mode need to create subnets manually. automode at times causes overlapping subnets when connecting vpcs for vpn/peering. automode vpcs cannot be peered
      - subnet reserved addresses - 0 - network, 1 - gateway, 254 - reserved, 255 - broadcast.
      - subnet primary address range for vm, secondary address range for multiple vms or containers on machine. 
      - subnet range can be expanded e.g. from /20 to /16 but not shrink.
    - ip_address:
      - internal dns names e.g. instance1.c.<project_id>.internal. ephemeral. assigned from  dhcp pool.
      - external ip address is optional. ephemeral. assigned when running. released on stop/delete. reserved (static) billed when not in used also.
      - no default external dns. cloud DNS service to be used.
      - multiple ips (nics) per vm desired for cases like control plane, data plane seperation. upto 2 cpus - 2 nics. thereafter 1 nic per cpu upto 8 nics.
    - routing:
      - defines paths of network traffic from vpc resource to other destinations. each route has a single destination + single next hop.
      - subnet routes cannot be deleted. custom routes can be created manually or by cloud router.
      - public and private nat. public nat provides public internet connectivity to resources without public ip. private nat may be used for connectivity 
      - between vpcs with overlapping subnets. clout nat is not a separate device or vm. it runs some process on each vm. source and destination (snat/dnat) is 
      - done by the software. 
      - for public nat - 1. create cloud router for vpc and region 2. create nat gateway for the cloud router for selected or all subnets 
      - 3. check the routes for the subnet. they would go thru nat gateway.
    - security:
      - roles - networkadmin (network resources crud), securityadmin (firewall, sslcert crud), computeinstanceadmin (gce crud), networkuser, networkviewer
      - connect to windows vm over rdp, iap (identity-aware-proxy tcp forwarding) client, sac(special administrative console over interactice serial console)
      - user need to have roles/iap.tunnelResourceAccessor role for iap access to resource. roles/compute.instanceadmin.v1 for ssh access.
      - iap access enables remote access without vm having public ip. iap access can be assigned using iam to specific users(,projects, folders etcs.) 
      - for specific resources (vm instances). firewall rules can enable iap traffic to vms on vpc.
    examples:
    - compute networks create n1 --subnet-mode=custom # create custom network n1
    - compute networks subnets create n1 --region=us-central1 --network=n1 --range=10.1.2.0/24 # create a subnet in network n1 for region us-central1
    - compute firewall-rules create n1-allow-ssh --allow=tcp:22 --priority=1000 --source-ranges=35.235.240.0/20 --network=n1 # enable ssh-from-browser from gcp console for n1 network
    - compute instances vm1 --network=n1 --subnet=n1 --zone=us-central1-c # create a vm connected to network n1
    - # use google console to ssh-from-browser to VM vm1
    - compute networks subnets expand-ip-range n1 --region=us-central1 --prefix-length=20 # expand ip range to 10.1.0.0/20 for the subnet
    - compute instances delete vm1 --zone=us-central1-c # delete the vm
    - compute networks subnets delete n1 --region=us-central1 # delete subnet n1 in region us-central1
    - compute networks delete n1 # delete custom network n1
    - compute addresses create public-ip1 # create static external ip address
    - compute instances create vm1 --zone=us-central1-c --machine-type=e2-small --network-interface=network=n1,subnet=n1 --network-interface=network=n2,subnet=n2,address=public-ip1 # create vm with two nics connected to two networks and static public-ip1
    - compute firewall-rules create n1-allow-ping2 --allow=icmp --target-tags=allow-ping --source-ranges=172.16.1.0/24 --network=n1 # allow ping to targets tagged allow-ping from given source ranges on network n1
    - compute firewall-rules create n1-deny-ping2 --direction=out --rules=icmp --action=deny --target-tags=deny-ping  --network=n1 --priority=999 # deny ping egress (out) from targets tagged deny-ping
    - compute ssh vm1 # ssh to vm1
    - compute routers  create cr1 --project=$P --network=n1 --region=us-central1 #create cloud router for network n1
    - compute routers nats create cn1 --router=cr1 --region=us-central1 --auto-allocate-nat-external-ips --nat-all-subnet-ip-ranges --enable-logging
    - compute instances create vm1 --machine-type=e2-small --zone=us-central1-f --network-interface=network=n1,subnet=n1,no-address # create vm without external ip on network n1
    - add firewall rules to allow ssh to vm; ssh to vm; curl http://google.com # vm without external ip able to access internet through nat
- pubsub:
    notes:
    - pricing starts at usd40 per tib
    - roles/pubsub.admin/editor/publisher/subscriber/viewer
    examples:
    - pubsub topics create tp1 # create topic tp1
    - pubsub subcriptions create stp1 --topic=tp1 # create subscription stp1 for topic tp1
    - pubsub topics publish tp1 --message="hi tp1" # public message to topic tp1
    - pubsub subcriptions pull stp1 # pull message from subscription stp1
